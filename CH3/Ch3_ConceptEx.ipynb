{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150e96e4",
   "metadata": {},
   "source": [
    "## *3.7 Conceptual Exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c7a51",
   "metadata": {},
   "source": [
    "## *1)*\n",
    "\n",
    "### t-statistics in table 3.4 are computed individually for each indicator/coefficient so we have 4 possible null hypothessis: \n",
    "\n",
    "### - $H_0$ : No rel. between TV and sales in the presence of Radio and newspaper ads (held fixed)\n",
    "### - $H_0$ : No rel. between Radio and sales in the presence of TV and newspaper ads  (held fixed)\n",
    "### - $H_0$ : No rel. between Newspaper and sales in the presence of TV and Radio ads  (held fixed)\n",
    "### - $H_0$ : No intercept --> in the absence of Newspaper, TV and Radio ads, average sales are zero. \n",
    "\n",
    "### p-values tells us that there is a relationship between TV/Radio and Sales and that we can reject the first two $H_0$. Also, since the p-value (probability) associated with the $\\beta_0 = 0$ null hypothesis (the last one) is small, we can reject the 4th null hypothesis. p-value for the 3rd $H_0$ is large implying there is no statistically significant relationship between Newspaper ads and Sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43064fc",
   "metadata": {},
   "source": [
    "## *2)* \n",
    "\n",
    "### KNN clasifier determines K nearest neighboring points in the feature space to make a prediction on the target based on which classs occurs the most within that neighborhood. \n",
    "\n",
    "### While KNN regressor takes an average of the target within the neighborhood to make a prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cade21b",
   "metadata": {},
   "source": [
    "## *3)*\n",
    "\n",
    "### Y = starting salary after graduation \n",
    "\n",
    "### The linear model with predictors is given by \n",
    "\n",
    "### $Y = \\beta_0 + \\beta_1 (\\textrm{GPA}) + \\beta_2 (\\textrm{IQ}) + \\beta_3 (\\textrm{Ed. Level}) + \\beta_4 (\\textrm{GPA}\\times\\textrm{IQ}) + \\beta_5 (\\textrm{GPA} \\times \\textrm{Ed. Level})$\n",
    "\n",
    "### Estimates for the parameters: \n",
    "\n",
    "### $\\hat{\\beta}_0 = 50,\\quad \\hat{\\beta}_1 = 20,\\quad \\hat{\\beta}_2 = 0.07,\\quad \\hat{\\beta}_3 = 35,\\quad \\hat{\\beta}_4 = 0.01,\\quad \\hat{\\beta}_5 = -10  $\n",
    "\n",
    "### - Ed.Level takes on 1 or 0 depending on College/High School Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812470e",
   "metadata": {},
   "source": [
    "### a) $Y_{col} - Y_{hs} = \\beta_3 - \\beta_5\\, \\textrm{GPA} = 35 - 10\\, \\textrm{GPA} $, so high school graduates earn more if $\\textrm{GPA} \\geq 3.5$. i, ii and iv do not hold in general for all 'fixed' GPA values so iii) is the correct statement. \n",
    "\n",
    "### b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e338476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted starting salary of a college graduate with a 4.0 GPA and 110 IQ is 137.1K $\n"
     ]
    }
   ],
   "source": [
    "GPA = 4.0\n",
    "IQ = 110\n",
    "def salary_calc(gpa,iq,Ed):\n",
    "    return 50 + 20 * (gpa) + 0.07 * (iq) + 35 * Ed + 0.01 * (gpa * iq) - 10 * (gpa * Ed)\n",
    "\n",
    "print(f'Predicted starting salary of a college graduate with a {GPA} GPA and {IQ} IQ is {salary_calc(4,110,1)}K $')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940d469",
   "metadata": {},
   "source": [
    "### c) False. Predicted value of a feature coefficient do not tell us about the statistical significance for the relevance of this feature in the Multi LR model. A t-statistic or F statistics is required to truly determine the relevance of the feature (and hence the coefficient $\\beta_4$). Equivalently one can have a look at the confidence interval of the predicted coefficient $\\beta_4$, if it contains 0. then it is likely that an interaction effect has any significance in the performence of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db7c94",
   "metadata": {},
   "source": [
    "## *4)* \n",
    "\n",
    "### a) As it is more flexible, the cubic model is expected to fit the training data better so that its RSS on the training set will be lower than the simple linear model. This is true irrespective of the true population following a linear relationship. \n",
    "\n",
    "### b) Since the true population exhibit a linear relationship, we expect the linear model to generalize better to the unseen data and so will have both a lower variance and low bias. Cubic model on the other hand will exhibit high variance, and hence will perform worse on the unseen data. Therefore RSS for the linear model is expected to be lower on the test set.   \n",
    "\n",
    "### c) As in our answer in part (a), cubic model has more degree of freedom to be able to model the data better than the linear model, leading to smaller RSS on the training set. \n",
    "\n",
    "### d) We need to know more about the degree of non-linearity of the true population to be able to give an answer. For example, if the true relationship is close to linear, the linear model could do well fitting the data with a low bias. In this case, the cubic model likely to have a high variance, leading to large RSS in the test set. If on the other hand, the true population relationship is significantly non-linear, the linear model will not be able to perform well on the test set, leading to large RSS as compared to cubic model. This is because since the true relationship is non-linear, cubic model should be able to model the data well without high variance (over-fitting) issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fde792",
   "metadata": {},
   "source": [
    "## *5)* \n",
    "\n",
    "### Considering Linear Regression without an intercept\n",
    "\n",
    "### $$\\hat{y}_i = x_i \\hat{\\beta}, \\quad\\quad \\hat{\\beta} = \\frac{\\sum_{\\lambda = 1}^{n} x_\\lambda y_\\lambda}{\\sum_{j=1}^{n} x_j^2} $$\n",
    "\n",
    "### Plug $\\hat{\\beta}$ in the prediction formula \n",
    "\n",
    "### $$ \\hat{y}_i = x_i \\frac{\\sum_{\\lambda = 1}^{n} x_\\lambda y_\\lambda}{\\sum_{j=1}^{n} x_j^2} = \\sum_{\\lambda = 1}^{n} \\left(\\frac{x_i x_\\lambda}{\\sum_{j=1}^{n} x_j^2}\\right) y_\\lambda \\quad \\longrightarrow \\quad a_\\lambda = \\frac{x_i x_\\lambda}{\\sum_{j=1}^{n} x_j^2}$$\n",
    "\n",
    "### So that each fitted value $\\hat{y}_i$ is given by the weighted linear sum of all actualy response values $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df5ab9",
   "metadata": {},
   "source": [
    "## *6)* \n",
    "\n",
    "### If the least squares line passes through the point $(\\bar{x},\\bar{y})$, the following equation should be satisfied \n",
    "\n",
    "### $$ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "### and since $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$, one can clearly see that it is indeed trivially satisfied on the least square regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649663e5",
   "metadata": {},
   "source": [
    "## *7)* \n",
    "\n",
    "### We need to prove $\\rm{Corr}(X,Y)^2 = R^2$ assuming $\\bar{x} = \\bar{y} = 0$\n",
    "\n",
    "### The latter assumption also implies that $\\hat{\\beta}_0 =0$ and \n",
    "### $$\\hat{\\beta}_1 = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}.$$\n",
    "\n",
    "### Now the $R^2$ score is given by\n",
    "### $$R^2 = 1 - \\frac{\\rm{RSS}}{\\rm{TSS}} = 1 - \\frac{\\sum_i (y_i - 0 - \\hat{\\beta}_1 x_i)^2}{\\sum_i (y_i - 0)^2} = \\frac{2\\hat{\\beta}_1 \\sum x_i y_i - \\hat{\\beta}_1^2 \\sum x_i^2}{\\sum y_i^2} = \\frac{\\left(\\sum x_i y_i\\right)^2}{\\sum x_i^2 \\sum y_i^2},$$\n",
    "\n",
    "### where in the last equality we plugged the expression for $\\hat{\\beta}_1$ above. On the other hand, for vanishing sample means with $\\bar{x} = \\bar{y} = 0$, the correlation is given by \n",
    "\n",
    "### $$ \\rm{Corr}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2}\\sqrt{\\sum (y_i - \\bar{y})^2}} \\quad \\longrightarrow \\quad \\frac{\\sum x_i y_i}{\\sqrt{\\sum (x_i^2}\\sqrt{\\sum y_i^2}}, $$\n",
    "\n",
    "### which is sufficient to prove $\\rm{Corr}(X,Y)^2 = R^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
