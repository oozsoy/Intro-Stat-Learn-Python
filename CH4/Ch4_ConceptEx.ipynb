{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 - Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Prove that 4.3 follows from 4.2, i.e the logistic regression function has logit that is linear in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$  p(X) = \\frac{\\mathrm{e}^{Y}}{1 + \\mathrm{e}^{Y}},\\quad\\quad Y = \\beta_0 + \\beta_1 X$$\n",
    "\n",
    "### we then have\n",
    "\n",
    "### $$ 1-p(X) = 1 - \\frac{\\mathrm{e}^{Y}}{1 + \\mathrm{e}^{Y}}  = \\frac{1}{1 + \\mathrm{e}^{Y}}$$\n",
    "\n",
    "which gives\n",
    "\n",
    "### $$ \\textrm{odds} = \\frac{p(X)}{1-p(X)} = \\frac{\\mathrm{e}^{Y}}{1 + \\mathrm{e}^{Y}}\\, (1 + \\mathrm{e}^{Y}) = e^{\\beta_0 + \\beta_1 X} $$\n",
    "\n",
    "### so that large odds between $0 \\leq \\textrm{odds} \\leq \\infty$ imply high probability in the logistic regression function $p(X)$. Notice that log odds / logit is linear in $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming observations that belong to a particular class are normally distributed, the posterier probability of that an observation $X = x$ belongs to the k-th class is given by \n",
    "\n",
    "### $$ p_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(\\frac{1}{2 \\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}.$$ We want to show that the class k for which the probability above is the largest is equivalent to classifiying an observation to the class for which the following discriminant function is maximized: $$ \\delta_k(x)=x \\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2 \\sigma^2}+\\log \\left(\\pi_k\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to maximize a probability with respect to a class $k$. It is clear from the form posterier probability that it will be maximized when the numerator is maximal for a given $k$. Taking the log of the numerator, we can reach at the same qualitative result since log transformation is a monotonic function that will not change the k at which the probability is maximal: $$ \\log(\\textrm{Numerator}) = \\log \\left(\\pi_k\\right)-\\log (\\sqrt{2 \\pi} \\sigma)-\\frac{1}{2 \\sigma^2}\\left(x-\\mu_k\\right)^2 $$, taking the square, we can drop all terms that do not depend on $k$, since they do will not affect the maximum. Essentially we are making an argmax operation with respect to k: \n",
    "### $$ \\textrm{arg max}_k (p_k(x)) \\equiv \\textrm{arg max}_k \\log(\\textrm{Numerator}) = \\textrm{arg max}_k (x \\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2 \\sigma^2}+\\log \\left(\\pi_k\\right))  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **p=1 LDA with a Gaussian distribution for predictor in each class both with a class dependent means and variances:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlike the previous example now we have observations within each class whose probability distribution is a Gaussian both with a class specific mean $\\mu_k$ and variance $\\sigma_k$. In this case the posterier distribution is given by:\n",
    "\n",
    "### $$ \\textrm{Pr}(Y = k | X = x) \\equiv p_k(x) =  \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma_k} \\exp \\left(-\\frac{1}{2 \\sigma_k^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi} \\sigma_l} \\exp \\left(\\frac{1}{2 \\sigma_l^2}\\left(x-\\mu_l\\right)^2\\right)} .$$ Following the same arguments of the previous exercise, each term in the log of the numerator is important for determining the k at which it becomes maximal: $$\\textrm{arg max}_k (p_k(x)) \\equiv \\textrm{arg max}_k \\log(\\textrm{Numerator}) = \\textrm{arg max}_k (x \\cdot \\frac{\\mu_k}{\\sigma_k^2}-\\frac{\\mu_k^2}{2 \\sigma_k^2}-x^2 \\cdot \\frac{1}{2 \\sigma_k^2} + \\log \\left(\\pi_k\\right)).$$ \n",
    "\n",
    "### Therefore in this case, Bayes classifier is not linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) **The curse of dimensionality in KNN and other local approaches:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Since X is uniformly distributed, i.e there is an equal amount of the occurance of X in the range [0,1]. If we want to make a prediction using a range within the 10 percent of any X, we will be on average using 10 % of the all available observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) For p = 2 and when (X_1, X_2) both uniformly distributed within [0,1]: if we wish to predict a test observation's (X_t = (X_1t, X_2t)) response using only observations that are within the 10% of the range of X_1 and within 10 % of the X_2 closest to that test observation, we are looking for an intersection in the X_1 and X_2 plane that has the required properties we described above. An intersection will be even rarer than its constitutents, since we are now looking for points (that satisfies those criterions) that are closest to that test point on a plane, these points should constitute, $0.1^2 = 0.01 \\equiv 1 \\%$ fraction of the all available observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) With the same logic, we expect to use $10^{-p}$ fraction of the all available observations, i.e $ 10^{-100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) The discussion above directly indicates that when the number of predictors p is large, there is very little training data for a given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Again with the same logic we followed so far, the 'area' of the hypercube with a given p should contain the indicated amount of information contained in the training observations. If the hypercube contains 10 % of the training observations, its one side $l$ can be determined via: \n",
    "\n",
    "### $$ l^{p} = 0.1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 1-hypercube around the test observation that contains 10 percent of the training observations would have approximately a side length of 0.10 \n",
      "A 2-hypercube around the test observation that contains 10 percent of the training observations would have approximately a side length of 0.32 \n",
      "A 100-hypercube around the test observation that contains 10 percent of the training observations would have approximately a side length of 0.98 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pvals = [1,2,100]\n",
    "\n",
    "for p in pvals:\n",
    "\n",
    "    side = np.exp(np.log(0.1)/p)\n",
    "\n",
    "    print(f\"A {p}-hypercube around the test observation that contains 10 percent of the training observations would have approximately a side length of {side:.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) **Differences between LDA and QDA:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Recall that for a classification task, Bayes decision boundary (BDB) is similar in principle to the concept of the true population regression. I.e it is the most optimal boundary that seperate different classes. Setting this information aside, independent of this relationship, we know that a more flexible model will perform better on the training set. So we expect QDA to outperform LDA on the training set. On the other hand, since the BDB is linear, QDA will tend to overfit the test data and in this case we expect the LDA to perform better which does not expected to suffer much from bias due to its simple structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Again in dependent on the nature of BDB, we expect QDA to outperform LDA due to its flexibility. Since, the BDB is non-linear QDA will also perform better on the test set without suffering from overfitting. In this case, LDA would suffer from bias on the test set since it is a less flexible model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Less flexible models tend to perform better (on the test set) relative to more flexible ones when the sample size is small. This is because increasing the sample size will reduce variance in the sample. For example, for a linear BDB, LDA will have a better performence on the test set because QDA will try follow the varince in the data, resulting with overfitting. As we increase the sample size since the variance is reduced, QDA will improve relative to LDA. On the other hand, for a non-linear decision boundary, increasing the sample size will not improve the LDA performence on the test set as it is biased. QDA however, will also exprience reduction in variance while enjoying its flexibility to capture non-linearities in the data. \n",
    "\n",
    "### To summarize: A more flexible method will experience more accuracy improvement as compared to a less flexible model when the sample size becomes larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) False. As we mentioned before, QDA will suffer from overfittin in this case, while a simple model like LDA will be just right in the context of bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) \n",
    "\n",
    "### $$ \\hat{p}(X) = \\frac{\\mathrm{e}^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2}}{1 + \\mathrm{e}^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2}} $$\n",
    "\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that a student with a GPA of 3.5 who stuides for 40h to get an A in the statistics class is 0.38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logitproba(b0, b1, b2, x1, x2):\n",
    "    return np.exp(b0 + b1 * x1 + b2 * x2)/(1 + np.exp(b0 + b1 * x1 + b2 * x2))\n",
    "\n",
    " 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) We use logit or log odds: $$ \\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) =  \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2$$ to solve for $X_1$ in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get an A in the statistics class this student has to work 50.00 hours\n"
     ]
    }
   ],
   "source": [
    "def hourstudy(prob, b0, b1, b2, x2):\n",
    "    return (np.log(prob/(1-prob)) - b0 - b2 * x2)/b1\n",
    "\n",
    "print(f\"To get an A in the statistics class this student has to work {hourstudy(0.5, -6, 0.05, 1, 3.5):.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) **Predicting whether a company will issue a dividend (Y = Yes or No) or not this year based on last years percent profit X:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) We need to use Bayes theorem: $$ \\textrm{Pr}(Y = k | X = x) \\equiv p_k(x) =  \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(\\frac{1}{2 \\sigma^2}\\left(x-\\mu_l\\right)^2\\right)}. $$ where $\\mu_1 = 10,\\, \\mu_2 = 0$, with a common standard deviation $\\sigma = 6$ and with priors $\\pi_1 = 0.8$ and $\\pi_2 = 0.2$. The first class stands for Yes whereas the second one stands for No, so we have two classes $K = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that a company will issue a dividend this year given that its percent profit was 4% last year is 0.75\n"
     ]
    }
   ],
   "source": [
    "def N_dist(x , mean , std):\n",
    "    prob_density = ((np.sqrt(2*np.pi)*std)**(-1)) * np.exp(-0.5*((x-mean)/std)**2)\n",
    "    return prob_density\n",
    "\n",
    "prior_vals = [0.8,0.2]\n",
    "mean_vals = [10.,0.]\n",
    "\n",
    "\n",
    "def post_dist(x, prior, mean, std):\n",
    "\n",
    "    denom = sum(prior_vals[i]*N_dist(x,mean_vals[i],6.) for i in range(2))\n",
    "\n",
    "    return prior * N_dist(x, mean, std)/denom\n",
    "\n",
    "print(f\"The probability that a company will issue a dividend this year given that its percent profit was 4% last year is {post_dist(4.,prior_vals[0],mean_vals[0],6):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model selection, the best practice is to always pick the model with the lowest test error rate. For logistic regression it is given by $\\epsilon^{LR}_{test} = 0.3$. For the KNN classifier with $N = 1$ we know the average of training and test error as 0.18 which implies:   $$ \\epsilon^{1NN}_{test} +  \\epsilon^{1NN}_{train} = 0.36$$\n",
    "### The crucial point about KNN with $K = 1$ is that, since it will choose the closest training sample to your test sample, it will always assign itself as the closest when it sees the training data as a test sample and will never make a mistake: $ \\epsilon^{1NN}_{train} = 0 $.\n",
    "### Therefore we have $$ \\epsilon^{LR}_{test} < \\epsilon^{1NN}_{test} $$ and hence we should prefer **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) $$ \\textrm{odds} = \\frac{p}{1-p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Average every 27 people out of 100 is expected to default\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "odds = 0.37\n",
    "\n",
    "prob = np.round(odds/(1 + odds),2)\n",
    "\n",
    "num = Fraction(prob).limit_denominator().numerator\n",
    "denum = Fraction(prob).limit_denominator().denominator\n",
    "\n",
    "print(f\"On Average every {num} people out of {denum} is expected to default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odds that she will default is 0.19\n"
     ]
    }
   ],
   "source": [
    "odds = 0.16/(1-0.16)\n",
    "\n",
    "print(f\"The odds that she will default is {odds:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) In LDA with a single predictor p = 1, log odds of assigning an observation to a class $k$ with respect to a baseline class $K$ is:  $$ \\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) + \\frac{1}{2\\sigma^2} \\left((x - \\mu_K)^2 - (x - \\mu_k)^2\\right) $$\n",
    "\n",
    "### where we used the Bayes Theorem. Playing with the squares in the second term noticing the $a^2 - b^2 = (a-b)(a+b)$ form, we get \n",
    "\n",
    "### $$ \\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = a_k + b_k\\, x $$ where $$ a_k  = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{\\mu_k^2 - \\mu_K^2}{2\\sigma^2},\\quad\\quad b_k = \\frac{\\mu_k - \\mu_K}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) For QDA, we covariance metrix of different classes are not equivalent $\\Sigma_k \\neq \\Sigma_K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) -\\frac{1}{2} \\log\\left(\\frac{|\\Sigma_k|}{|\\Sigma_K|}\\right)  - \\frac{1}{2} (x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) + \\frac{1}{2} (x-\\mu_K)^T \\Sigma_K^{-1} (x-\\mu_K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first distributing the quadratic forms using $(x-\\mu)^T = (x^T - \\mu^T)$. Then use the fact that the following quantity is scalar so that $x^T \\sigma^{-1} \\mu = \\mu^T \\Sigma^{-1} x$ which can be confirmed by transposing the left hand side and using the fact that for a symmetric matrix $(\\Sigma^{-1})^T = \\Sigma^{-1}$. This gives: \n",
    "\n",
    "### $$\\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) -\\frac{1}{2} \\log\\left(\\frac{|\\Sigma_k|}{|\\Sigma_K|}\\right) + \\frac{1}{2} x^T (\\Sigma_K^{-1}-\\Sigma_k^{-1})\\, x + (\\mu_k^T \\Sigma_k^{-1} - \\mu_K^T \\Sigma_K^{-1})\\, x + \\frac{1}{2} (\\mu_K^T \\Sigma_K^{-1}\\mu_K - \\mu_k^T \\Sigma_k^{-1}\\mu_k)$$\n",
    "\n",
    "### which reduces to the LDA case when $\\Sigma_k = \\Sigma_K$. For the expression above we can write\n",
    "\n",
    "### $$\\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = a_k + \\sum_{j=1}^p b_{kj} x_j + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl} x_j x_l$$\n",
    "\n",
    "### and infer the coefficients a,b,c in terms of the priors, covariance matrices and means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) a) and b)\n",
    "\n",
    "### For logistic regression the log odds of orange versus apple is simply given by $$ \\log\\left(\\frac{p}{1-p}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$ whereas for the softmax formulation the same log odds ratio is given by $$\\log\\left(\\frac{\\textrm{Pr}(Y = k | X = x) }{\\textrm{Pr}(Y = K | X = x) }\\right) = (\\hat{\\alpha}_{{\\rm orange}0} - \\hat{\\alpha}_{{\\rm apple}0}) + (\\hat{\\alpha}_{{\\rm orange}1} - \\hat{\\alpha}_{{\\rm apple}1})x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Since the interpretation of the log odds the same in both formulations we have $$ \\hat{\\alpha}_{{\\rm orange}0} - \\hat{\\alpha}_{{\\rm apple}0} = \\hat{\\beta}_0 = 1.2 $$ and $$ \\hat{\\alpha}_{{\\rm orange}1} - \\hat{\\alpha}_{{\\rm apple}1} =  \\hat{\\beta}_1 = -1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) If we are fitting the two models to the same data we could extract the expected coefficients in our model according to the formulas above. Which gives $ \\hat{\\beta}_0 = -2.2$ and $\\hat{\\beta}_1 = -2.6$. If we are fitting to different data sets we can not extract this information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Essentially interpretation of our results is the same, regardsless of coding. The decision boundary of the two model will be same and so we expect perfect agreement for the predicted class labels of both models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
